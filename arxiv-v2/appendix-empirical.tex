In this section, we derive \cref{alg:empest} and prove
\cref{thm:empirical}.

\subsection{Estimators for $\vpi$ and $\gap$}

The algorithm forms the estimator $\wh\vP$ of $\vP$ using Laplace smoothing:
\[
  \wh{P}_{i,j}
  := \frac{N_{i,j} + \alpha}{N_i + d\alpha}
\]
where
\[
  N_{i,j} := \Abs{ \Braces{ t \in [n-1] : (X_t,X_{t+1}) = (i,j) } } ,
  \quad
  N_i := \Abs{ \Braces{ t \in [n-1] : X_t = i } }
\]
and $\alpha>0$ is a positive constant, which we set beforehand as $\alpha := 1/d$
for simplicity.

As a result of the smoothing, all entries of $\wh\vP$ are positive,
and hence $\wh\vP$ is a transition probability
matrix for an ergodic Markov chain.
We let $\hat\vpi$ be the unique stationary distribution for $\wh\vP$.
Using $\hat\vpi$, we form an estimator $\Sym(\wh\vL)$ of $\vL$ using:
\[
  \Sym(\wh\vL) := \frac12 \parens{ \wh\vL + \wh\vL^\t }
  ,
  \qquad
  \wh\vL := \Diag(\hat\vpi)^{1/2} \wh\vP \Diag(\hat\vpi)^{-1/2}
  .
\]
Let $\hat\lambda_1 \geq \hat\lambda_2 \geq \dotsb \geq \hat\lambda_d$
be the eigenvalues of $\Sym(\wh\vL)$ (and in fact, we have $1 =
\hat\lambda_1 > \hat\lambda_2$ and $\hat\lambda_d > -1$).
The algorithm estimates the spectral gap $\gap$ using
\[
  \hatgap := 1 - \max\{ \hat\lambda_2, |\hat\lambda_d| \} .
\]

\subsection{Empirical bounds for $\vP$}
\label{sec:P-obs-bound}

We make use of a simple corollary of Freedman's inequality for
martingales~\citep[Theorem 1.6]{Fre75}.
\begin{theorem}[Freedman's inequality]
  \label{thm:freedman}
  Let $(Y_t)_{t \in \bbN}$ be a bounded martingale difference sequence
  with respect to the filtration $\cF_0 \subset \cF_1 \subset \cF_2
  \subset \dotsb$; assume for some $b>0$, $|Y_t| \leq b$ almost surely
  for all $t \in \bbN$.
  Let $V_k := \sum_{t=1}^k \EE{Y_t^2|\cF_{t-1}}$ and $S_k :=
  \sum_{t=1}^k Y_t$ for $k \in \bbN$.
  For all $s, v > 0$,
  \[
    \Pr\Brackets{
      \exists k \in \bbN \;\st\,
      S_k > s
      \,\wedge\,
      V_k \leq v
    }
    \leq \Parens{
      \frac{v}{s+v}
    }^{s+v}
    e^s
    = \exp\Parens{
      -\frac{v}{b^2} \cdot h\Parens{\frac{bs}{v}}
    }
    \,,
  \]
  where $h(u) := (1+u)\ln(1+u) - u$.
\end{theorem}
Observe that in \Cref{thm:freedman}, for any $x>0$, if $s :=
\sqrt{2vx} + bx/3$ and $z := b^2x/v$, then probability bound becomes
\[
  \exp\Parens{
    -x \cdot \frac{h\Parens{\sqrt{2z}+z/3}}{z}
  }
  \leq
  e^{-x}
\]
since $h(\sqrt{2z}+z/3)/z \geq 1$ for all $z > 0$ (see,
e.g.,~\citep[proof of Lemma 5]{audibert2009}).

\begin{corollary}
  \label{cor:freedman}
  Under the same setting as \Cref{thm:freedman}, for any $n \geq 1$,
  $x > 0$, and $c > 1$,
  \[
    \Pr\Brackets{
      \exists k \in [n] \;\st\,
      S_k > \sqrt{2cV_kx} + 4bx/3
    }
    \leq
    \Parens{ 1 + \ceil{\log_c(2n/x)}_+ }
    e^{-x}
    .
  \]
\end{corollary}
\begin{proof}
  Define $v_i := c^i b^2x/2$ for $i = 0, 1, 2, \dotsc, \ceil{
  \log_c(2n/x) }_+$, and let $v_{-1} := -\infty$.
  Then, since $V_k \in [0,b^2n]$ for all $k\in[n]$,
  \begin{align*}
    \MoveEqLeft{
      \Pr\Brackets{
        \exists k \in [n] \;\st\,
        S_k > \sqrt{2\max\braces{v_0,cV_k}x} + bx/3
      }
    }
    \\
    & =
    \sum_{i=0}^{\ceil{\log_c(2n/x)}_+}
    \Pr\Brackets{
      \exists k \in [n] \;\st\,
      S_k > \sqrt{2\max\braces{v_0,cV_k}x} + bx/3
      \,\wedge\, v_{i-1} < V_k \leq v_i
    }
    \\
    & \leq
    \sum_{i=0}^{\ceil{\log_c(2n/x)}_+}
    \Pr\Brackets{
      \exists k \in [n] \;\st\,
      S_k > \sqrt{2\max\braces{v_0,cv_{i-1}}x} + bx/3
      \,\wedge\, v_{i-1} < V_k \leq v_i
    }
    \\
    & \leq
    \sum_{i=0}^{\ceil{\log_c(2n/x)}_+}
    \Pr\Brackets{
      \exists k \in [n] \;\st\,
      S_k > \sqrt{2v_ix} + bx/3
      \,\wedge\, V_k \leq v_i
    }
    \\
    & \leq
    \Parens{ 1 + \ceil{\log_c(2n/x)}_+ }
    e^{-x}\,,
  \end{align*}
  where the final inequality uses \Cref{thm:freedman}.
  The conclusion now follows because
  \[
    \sqrt{2cV_kx} + 4bx/3
    \geq \sqrt{2\max\braces{v_0,cV_k}x} + 2bx/3
  \]
  for all $k \in [n]$.
\end{proof}

\begin{lemma}
  \label{lem:P-unobs-bound}
  The following holds for any constant $c>1$ with probability at least
  $1-\delta$: for all $(i,j) \in \iset{d}^2$,
  \begin{equation}
    \abs{ \wh{P}_{i,j} - P_{i,j} }
    \leq
    \sqrt{\Parens{\frac{N_i}{N_i+d\alpha}}\frac{2cP_{i,j}(1-P_{i,j})\emptail}{N_i+d\alpha}}
    + \frac{(4/3)\emptail}{N_i+d\alpha}
    + \frac{\abs{\alpha-d\alpha P_{i,j}}}{N_i+d\alpha}\,,
    \label{eq:P-unobs-bound}
  \end{equation}
  where
  \begin{equation}
    \emptail
    := \inf
    \Braces{
      t\geq0 :
      2d^2 \Parens{ 1 + \ceil{\log_c(2n/t)}_+ } e^{-t} \leq \delta
    }
    = O\Parens{ \log\Parens{ \frac{d\log(n)}{\delta} } }
    \,.
    \label{eq:emptail}
  \end{equation}
\end{lemma}
\begin{proof}
  Let $\cF_t$ be the $\sigma$-field generated by $X_1,X_2,\dotsc,X_t$.
  Fix a pair $(i,j) \in [d]^2$.
  Let $Y_1 := 0$, and for $t \geq 2$,
  \[
    Y_t := \Ind{X_{t-1} = i} (\Ind{X_t=j} - P_{i,j})
    ,
  \]
  so that
  \[
    \sum_{t=1}^n Y_t
    = N_{i,j} - N_i P_{i,j}
    .
  \]
  The Markov property implies that the stochastic process
  $(Y_t)_{t\in[n]}$ is an $(\cF_t)$-adapted martingale difference
  sequence: $Y_t$ is $\cF_t$-measurable and $\EE{Y_t| \cF_{t-1} } =
  0$, for each $t$.
  Moreover, for all $t \in [n]$,
  \[
    Y_t \in [-P_{i,j},1-P_{i,j}]
    \,,
  \]
  and for $t \geq 2$,
  \[
    \EE{Y_t^2| \cF_{t-1} } = \Ind{X_{t-1}=i} P_{i,j} (1-P_{i,j})
    \,.
  \]
  Therefore, by \Cref{cor:freedman} and union bounds, we have
  \[
    \abs{ N_{i,j} - N_i P_{i,j} }
    \leq \sqrt{2c N_i P_{i,j} (1-P_{i,j}) \emptail} +
    \frac{4\emptail}3
  \]
  for all $(i,j) \in [d]^2$.
\end{proof}

\Cref{eq:P-unobs-bound} can be viewed as constraints on the possible
value that $P_{i,j}$ may have (with high probability).
Since $P_{i,j}$ is the only unobserved quantity in the bound from
\cref{eq:P-unobs-bound}, we can numerically maximize $|\wh{P}_{i,j} -
P_{i,j}|$ subject to the constraint in \cref{eq:P-unobs-bound}
(viewing $P_{i,j}$ as the optimization variable).
Let $B_{i,j}^*$ be this maximum value, so we have
\[
  P_{i,j} \in
  \Brackets{
    \wh{P}_{i,j} - B_{i,j}^*,\,
    \wh{P}_{i,j} + B_{i,j}^*
  }
\]
in the same event where \cref{eq:P-unobs-bound} holds.

In the algorithm, we give a simple alternative to computing
$B_{i,j}^*$ that avoids numerical optimization, derived in the spirit
of empirical Bernstein bounds~\citep{audibert2009}.
Specifically, with $c := 1.01$ (an arbitrary choice), we compute
\begin{equation}
  \wh{B}_{i,j}
  :=
  \Parens{
    \sqrt{\frac{c\emptail}{2N_i}}
    + \sqrt{
      \frac{c\emptail}{2N_i}
      +
      \sqrt{\frac{2c\wh{P}_{i,j}(1-\wh{P}_{i,j})\emptail}{N_i}}
      + \frac{(4/3)\emptail + \abs{\alpha-d\alpha\wh{P}_{i,j}}}{N_i}
    }
  }^2
  \label{eq:P-obs-bound}
\end{equation}
for each $(i,j) \in [d]^2$, where $\emptail$ is defined in
\cref{eq:emptail}.
We show in \cref{lem:P-obs-bound} that
\[
  P_{i,j} \in
  \Brackets{
    \wh{P}_{i,j} - \wh{B}_{i,j},\,
    \wh{P}_{i,j} + \wh{B}_{i,j}
  }
\]
again, in the same event where \cref{eq:P-unobs-bound} holds.
The observable bound in \cref{eq:P-obs-bound} is not too far from the
unobservable bound in~\cref{eq:P-unobs-bound}.

\begin{lemma}
  \label{lem:P-obs-bound}
  In the same $1-\delta$ event as from \cref{lem:P-unobs-bound},
  we have
  $P_{i,j} \in \brackets{ \wh{P}_{i,j} - \wh{B}_{i,j},\, \wh{P}_{i,j}
  + \wh{B}_{i,j} }$
  for all $(i,j) \in [d]^2$,
  where $\wh{B}_{i,j}$ is defined in \cref{eq:P-obs-bound}.
\end{lemma}
\begin{proof}
  Recall that in the $1-\delta$ probability event from
  \cref{lem:P-unobs-bound}, we have for all $(i,j) \in [d]^2$,
  \begin{multline*}
    \abs{ \wh{P}_{i,j} - P_{i,j} }
    =
    \Abs{
      \frac{N_{i,j} - N_i P_{i,j}}{N_i + d\alpha}
      + \frac{\alpha - d\alpha P_{i,j}}{N_i + d\alpha}
    } %\\
    \leq
    \sqrt{\frac{2cN_iP_{i,j}(1-P_{i,j})\emptail}{(N_i+d\alpha)^2}}
    + \frac{(4/3)\emptail}{N_i+d\alpha}
    + \frac{\abs{\alpha-d\alpha P_{i,j}}}{N_i+d\alpha}
    .
  \end{multline*}
  Applying the triangle inequality to the right-hand side, we obtain
  \begin{align}
    \abs{ \wh{P}_{i,j} - P_{i,j} }
    & \leq
    \sqrt{
      \frac{
        2cN_i\parens{
          \wh{P}_{i,j}(1-\wh{P}_{i,j})
          + \abs{\wh{P}_{i,j} - P_{i,j}}
        } \emptail
      }{(N_i+d\alpha)^2}
    }
    + \frac{(4/3)\emptail}{N_i+d\alpha}
    \notag \\
    & \qquad
    + \frac{
      \abs{\alpha-d\alpha \wh{P}_{i,j}}
      + d\alpha\abs{\wh{P}_{i,j}-P_{i,j}}
    }{N_i+d\alpha}
    .
    \notag
  \end{align}
  Since $\sqrt{A+B} \leq \sqrt{A} + \sqrt{B}$ for non-negative $A,B$, we
  loosen the above inequality and rearrange it to obtain
  \begin{align*}
    \Parens{ 1 - \frac{d\alpha}{N_i+d\alpha} }
    \abs{\wh{P}_{i,j} - P_{i,j}}
    & \leq
    \sqrt{\abs{\wh{P}_{i,j} - P_{i,j}}} \cdot
    \sqrt{\frac{2cN_i\emptail}{(N_i+d\alpha)^2}}
    \\
    & \qquad
    +
    \sqrt{\frac{2cN_i\wh{P}_{i,j}(1-\wh{P}_{i,j})\emptail}{(N_i+d\alpha)^2}}
    + \frac{(4/3)\emptail + \abs{\alpha -
    d\alpha\wh{P}_{i,j}}}{N_i+d\alpha}
    .
  \end{align*}
  Whenever $N_i > 0$, we can solve a quadratic inequality to conclude
  $\abs{\wh{P}_{i,j}-P_{i,j}} \leq \wh{B}_{i,j}$.
\end{proof}

\subsection{Empirical bounds for $\vpi$}

Recall that $\hat\vpi$ is obtained as the unique stationary
distribution for $\wh\vP$.
Let $\wh\vA := \vI - \wh\vP$, and let $\giAh$ be the \emph{group
inverse} of $\wh\vA$---i.e., the unique square matrix satisfying the
following equalities:
\[
  \wh\vA\giAh\wh\vA = \wh\vA , \quad
  \giAh\wh\vA\giAh = \giAh , \quad
  \giAh\wh\vA = \wh\vA\giAh .
\]
The matrix $\giAh$, which is well defined no matter what transition probability matrix $\wh\vP$ we start with
\citep{meyer1975role},
 is a central quantity that captures many properties
of the ergodic Markov chain with transition matrix
$\wh\vP$~\citep{meyer1975role}.
We denote the $(i,j)$-th entry of $\giAh$ by $\giAh_{i,j}$.
Define
\begin{equation}
  \hat\kappa :=
  \frac12
  \max
  \Braces{
    \giAh_{j,j}
    - \min\Braces{ \giAh_{i,j} : i \in [d] }
    : j \in [d]
  }
  .
  \notag
\end{equation}
Analogously define
\begin{align*}
  \vA & := \vI - \vP , \\
  \giA & := \text{group inverse of $\vA$} , \\
  \kappa & :=
  \frac12
  \max
  \Braces{
    \giA_{j,j}
    - \min\Braces{ \giA_{i,j} : i \in [d] }
    : j \in [d]
  }
  .
\end{align*}
We now use the following perturbation bound from \citep[Section
3.3]{cho2001comparison} (derived
from~\citet{haviv1984perturbation,kirkland1998applications}).
\begin{lemma}[\citep{haviv1984perturbation,kirkland1998applications}]
  \label{lem:pi-perturb}
  If
  $\abs{\wh{P}_{i,j} - P_{i,j}} \leq \wh{B}_{i,j}$ for each $(i,j) \in
  [d]^2$, then
  \begin{align}
    \max\Braces{
      \abs{\hat\pi_i - \pi_i}
      : i \in [d]
    }
    & \leq \min\{\kappa,\hat\kappa\} \max
    \braces{
      \wh{B}_{i,j}
      : (i,j) \in [d]^2
    }
    \notag \\
    & \leq \hat\kappa \max
    \braces{
      \wh{B}_{i,j}
      : (i,j) \in [d]^2
    }
    .
    \notag
  \end{align}
\end{lemma}
This establishes the validity of the confidence intervals for the
$\pi_i$ in the same event from \cref{lem:P-unobs-bound}.

We now establish the validity of the bounds for the ratio quantities
$\sqrt{\hat\pi_i/\pi_i}$ and $\sqrt{\pi_i/\hat\pi_i}$.
\begin{lemma}
  \label{lem:pi-ratio-perturb}
  If $\max\braces{\abs{\hat\pi_i-\pi_i} : i \in [d]} \leq \hat{b}$,
  then
  \begin{equation}
    \max\bigcup_{i\in[d]}
    \braces{
      \abs{\sqrt{\pi_i/\hat\pi_i}-1},\,
      \abs{\sqrt{\hat\pi_i/\pi_i}-1}
    }
    \leq
    \frac12 \max \bigcup_{i\in[d]}
    \Braces{
      \frac{\hat{b}}{\hat\pi_i},\,
      \frac{\hat{b}}{\brackets{\hat\pi_i-\hat{b}}_+}
    }
    .
    \notag
  \end{equation}
\end{lemma}
\begin{proof}
  By \cref{lem:pi-perturb}, we have for each $i \in [d]$,
  \begin{equation}
    \frac{\abs{\hat\pi_i - \pi_i}}{\hat\pi_i}
    \leq \frac{\hat{b}}{\hat\pi_i}
    , \quad
    \frac{\abs{\hat\pi_i - \pi_i}}{\pi_i}
    \leq
    \frac{\hat{b}}{\pi_i}
    \leq
    \frac{\hat{b}}{\brackets{\hat{\pi}_i-\hat{b}}_+}
    .
    \notag
  \end{equation}
  Therefore, using the fact that for any $x>0$,
  \[
    \max\Braces{
      |\sqrt{x}-1| ,\, |\sqrt{1/x}-1|
    } \leq \frac12 \max\Braces{ |x-1| ,\, |1/x-1| }
  \]
  we have for every $i \in [d]$,
  \begin{align*}
    \max\Braces{
      \abs{\sqrt{\pi_i/\hat\pi_i}-1}
      ,\,
      \abs{\sqrt{\hat\pi_i/\pi_i}-1}
    }
    & \leq
    \frac12 \max\Braces{
      \abs{\pi_i/\hat\pi_i-1}
      ,\,
      \abs{\hat\pi_i/\pi_i-1}
    }
    \\
    & \leq
    \frac12 \max\Braces{
      \frac{\hat{b}}{\hat\pi_i}
      ,\,
      \frac{\hat{b}}{\brackets{\hat{\pi}_i-\hat{b}}_+}
    }
    .
    \qedhere
  \end{align*}
\end{proof}


\subsection{Empirical bounds for $\vL$}

By Weyl's inequality and the triangle inequality,
\[
  \max_{i \in [d]} |\lambda_i - \hat\lambda_i|
  \leq \norm{\vL - \Sym(\wh\vL)}
  \leq \norm{\vL - \wh\vL} .
\]
It is easy to show that $|\hatgap - \gap|$ is bounded by the same
quantity.
Therefore, it remains to establish an empirical bound on $\norm{\vL -
\wh\vL}$.

\begin{lemma}
  \label{lem:L-obs-bound}
  If
  $\abs{\wh{P}_{i,j} - P_{i,j}} \leq \wh{B}_{i,j}$ for each $(i,j) \in
  [d]^2$ and
  $\max\braces{\abs{\hat\pi_i-\pi_i} : i \in [d]} \leq \hat{b}$,
  then
  \[
    \norm{\wh\vL - \vL}
    \leq
    2\hat\rho + \hat\rho^2
    + (1+2\hat\rho+\hat\rho^2)
    \Biggl(
      \sum_{(i,j)\in[d]^2} \frac{\hat\pi_i}{\hat\pi_j} \hat{B}_{i,j}^2
    \Biggr)^{1/2}
    ,
  \]
  where
  \[
    \hat\rho := \frac12 \max \bigcup_{i\in[d]}
    \Braces{
      \frac{\hat{b}}{\hat\pi_i},\,
      \frac{\hat{b}}{\brackets{\hat\pi_i-\hat{b}}_+}
    }
    .
  \]
\end{lemma}
\begin{proof}
  We use the following decomposition of $\vL - \wh\vL$:
  \[
    \vL - \wh\vL
    = \errP
    + \errpil \wh\vL
    + \wh\vL \errpir
    + \errpil \errP
    + \errP \errpir
    + \errpil \wh\vL \errpir
    + \errpil \errP \errpir
  \]
  where
  \begin{align*}
    \errP
    & := \Diag(\hat\vpi)^{1/2} \parens{ \vP - \wh\vP } \Diag(\hat\vpi)^{-1/2} ,
    \\
    \errpil
    & := \Diag(\vpi)^{1/2} \Diag(\hat\vpi)^{-1/2} - \vI ,
    \\
    \errpir
    & := \Diag(\hat\vpi)^{1/2} \Diag(\vpi)^{-1/2} - \vI .
  \end{align*}
  Therefore
  \begin{align*}
    \norm{\vL - \wh\vL}
    & \leq
    \norm{\errpil} + \norm{\errpir} + \norm{\errpil} \norm{\errpir}
    \\
    & \quad
    + \Parens{
      1 + \norm{\errpil} + \norm{\errpir} + \norm{\errpil} \norm{\errpir}
    }
    \norm{\errP}
    .
  \end{align*}
  Observe that for each $(i,j) \in [d]^2$, the $(i,j$)-th entry of
  $\errP$ is bounded in absolute value by
  \[
    |(\errP)_{i,j}|
    = \hat{\pi}_i^{1/2} \hat{\pi}_j^{-1/2} |P_{i,j} - \wh{P}_{i,j}|
    \leq
    \hat\pi_i^{1/2} \hat{\pi}_j^{-1/2} \wh{B}_{i,j}
    .
  \]
  Since the spectral norm of $\errP$ is bounded above by its Frobenius
  norm,
  \[
    \norm{\errP}
    \leq \Biggl(
      \sum_{(i,j) \in [d]^2}
      (\errP)_{i,j}^2
    \Biggr)^{1/2}
    \leq \Biggl(
      \sum_{(i,j) \in [d]^2}
      \frac{\pi_i}{\pi_j} \wh{B}_{i,j}^2
    \Biggr)^{1/2}
    .
  \]
  Finally, the spectral norms of $\errpil$ and $\errpir$ satisfy
  \[
    \max\Braces{ \norm{\errpil} ,\, \norm{\errpir} }
    =
    \max\bigcup_{i\in[d]}
    \braces{
      \abs{\sqrt{\pi_i/\hat\pi_i}-1},\,
      \abs{\sqrt{\hat\pi_i/\pi_i}-1}
    }
    ,
  \]
  which can be bounded using \cref{lem:pi-ratio-perturb}.
\end{proof}

This establishes the validity of the confidence interval for $\gap$ in
the same event from \cref{lem:P-unobs-bound}.

\subsection{Asymptotic widths of intervals}
\label{sec:asymptotic}

Let us now turn to the asymptotic behavior of the interval widths
(regarding $\hat{b}$, $\hat\rho$, and $\hat{w}$ all as
functions of $n$).

A simple calculation gives that, almost surely, as $n\to \infty$,
\begin{align*}
  \sqrt{\frac{n}{\log\log n}}
  \hat{b}
  & =
  O\Parens{
    \max_{i,j} \kappa \sqrt{\frac{P_{i,j}}{\pi_i}}
  }
  ,
  \\
  \sqrt{\frac{n}{\log\log n}}
  \hat\rho
  & =
  O\Parens{
    \frac{\kappa}{\pimin^{3/2}}
  }
  .
\end{align*}
Here, we use the fact that $\hat\kappa \to \kappa$ as $n\to\infty$
since $\giAh \to \giA$ as $\wh\vP \to
\vP$~\citep{li2001improvement,benitez2012continuity}.

Further, since
\begin{align*}
  \sqrt{\frac{n}{\log\log n}}
  \Biggl(
    \sum_{i,j} \frac{\hat{\pi}_i}{\hat{\pi}_j} \wh{B}_{i,j}^2
  \Biggr)^{1/2}
  & =
  O\Parens{
    \Biggl(
      \sum_{i,j} \frac{{\pi}_i}{{\pi}_j}
      \cdot \frac{P_{i,j}(1-P_{i,j})}{\pi_i}
    \Biggr)^{1/2}
  }
  =
  O\Parens{ \sqrt{ \frac{d}{\pimin}} }
  \,,
\end{align*}
we thus have
\begin{align*}
  \sqrt{\frac{n}{\log\log n}}
  \hat{w}
  =
  O\Parens{
    \frac{\kappa}{\pimin^{3/2}} +
    \sqrt{\frac{d}{\pimin}}
  }
  .
\end{align*}
This completes the proof of \cref{thm:empirical}.
\hfill $\qed$

The following claim provides a bound on $\kappa$ in terms of the
number of states and the spectral gap.
\begin{claim}
  \label{claim:kappa-bound}
  $\kappa \leq d/\gap$.
\end{claim}
\begin{proof}
  It is established by Cho and Meyer~\citep{cho2001comparison} that
  \[
    \kappa
    \leq \max_{i,j} |\giA_{i,j}|
    \leq
    \sup_{\norm{\vv}_1 = 1 , \dotp{\vv,\v1} = 0} \norm{\vv^\t \giA}_1
  \]
  (our $\kappa$ is the $\kappa_4$ quantity from
  \citep{cho2001comparison}), and Seneta~\citep{seneta1993sensitivity}
  establishes
  \[
    \sup_{\norm{\vv}_1 = 1 , \dotp{\vv,\v1} = 0} \norm{\vv^\t \giA}_1
    \leq
    \frac{d}{\gap}
    .
    \qedhere
  \]
\end{proof}

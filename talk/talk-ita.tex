\documentclass[11pt,compress,blue4,notheorems]{beamer}
\usepackage[T1]{fontenc}
\usepackage{algorithmic}
\usepackage[normalem]{ulem}

\input{__includes}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{\hfill\insertframenumber\hspace{1mm}\vspace{1mm}}

\definecolor{boldgreen}{rgb}{0.2,0.6,0.2}
\definecolor{darkgreen}{rgb}{0.1,0.4,0.1}
\definecolor{darkblue}{rgb}{0,0,0.7}
\definecolor{firebrick}{rgb}{0.7,0.13,0.13}
\definecolor{britishracinggreen}{rgb}{0.0, 0.26, 0.15}
\definecolor{azure}{rgb}{0.0,0.5,1.0}
\definecolor{awesome}{rgb}{1.0, 0.13, 0.32}
\newcommand{\RED}[1]{\textcolor{red}{#1}}
\newcommand{\GREEN}[1]{\textcolor{boldgreen}{#1}}
\newcommand{\GRAY}[1]{\textcolor{gray}{#1}}
\newcommand{\LIGHTGRAY}[1]{\textcolor{lightgray}{#1}}
\newcommand{\BLUE}[1]{\textcolor{blue}{#1}}
\newcommand{\FIREBRICK}[1]{\textcolor{firebrick}{#1}}
\newcommand{\AZURE}[1]{\textcolor{azure}{#1}}
\newcommand{\AWESOME}[1]{\textcolor{awesome}{#1}}

\newcommand\fns\footnotesize

\newcommand\tv{\ensuremath{\operatorname{tv}}}
\newcommand\tmix{\ensuremath{t_{\operatorname{mix}}}}
\newcommand\tmixhat{\ensuremath{\hat{t}_{\operatorname{mix}}}}
\newcommand\trelax{\ensuremath{t_{\operatorname{relax}}}}
\newcommand\pimin{\ensuremath{\pi_{\star}}}
\newcommand\gap{\ensuremath{\gamma_{\star}}}
\newcommand\hatgap{\ensuremath{\hat{\gamma}_{\star}}}
\newcommand\slem{\ensuremath{\lambda_{\star}}}

\newcommand\states{\ensuremath{\mathcal{X}}}

\AtBeginSection[]
{\begin{frame}<beamer>
\begin{center}
{\Large \thesection. \ \insertsection}
\end{center}
\end{frame}}
\setbeamertemplate{section in toc}[sections numbered]

\title{Confidence intervals for the mixing time \\ of a reversible
Markov chain \\ from a single sample path}
\author{%
  Daniel Hsu$^\dag$ \and
  Aryeh Kontorovich$^\sharp$ \and
  Csaba Szepesv\'ari$^\star$%
}
\institute{%
  $^\dag$Columbia University,
  $^\sharp$Ben-Gurion University,
  $^\star$University of Alberta
}
\date{}

\begin{document}

\begin{frame}
  \titlepage

  \begin{center}
    ITA 2016
  \end{center}
\end{frame}

%-------------------------------------------------------------------------------
%
%\begin{frame}
%\frametitle{Outline}
%\tableofcontents[hideallsubsections]
%\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Problem}

  \begin{itemize}
    \item
      Irreducible, aperiodic, time-homogeneous Markov chain
      \[
        X_1 \ \to \ X_2 \ \to \ X_3 \ \to \ \dotsb
      \]

    \item<2->
      There is a unique \GREEN{stationary distribution} $\pi$ with
      \[
        \lim_{t\to\infty}
        \cL(X_t \mid X_1=x) \ = \ \pi
        \,,
        \quad \text{for all $x \in \states$}
        \,.
      \]

    \item<3->
      The \GREEN{mixing time} $\tmix$ is the earliest time $t$ with
      \[
        \sup_{x \in \states}
        \norm{\cL(X_t \mid X_1=x) - \pi}_{\tv}
        \ \leq \
        1/4
        \,.
      \]

  \end{itemize}

  \onslide<4->
  \textbf{Problem}:

  \medskip
  \begin{overprint}
    \onslide<4>
    \begin{center}
      Determine (confidently) if $t\geq\tmix$ after seeing $X_1,X_2,\dotsc,X_t$.
    \end{center}
    \onslide<5>
    \begin{center}
      Given $\delta \in (0,1)$ and $X_{1:t}$, determine non-trivial
      $I_t \subseteq [0,\infty]$ with
      \[
        \P(\tmix \in I_t) \ \geq \ 1-\delta
        \,.
      \]
    \end{center}
  \end{overprint}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Some motivation from machine learning and statistics}

  \textbf{Chernoff bounds for Markov chains $X_1\to X_2\to\dotsb$}:
  \\
  for suitably well-behaved $f \colon \states \to \bbR$,
  with probability at least $1-\delta$,
  \[
    \Abs{
      \frac1t\sum_{i=1}^t f(X_i)
      -
      \E_\pi f
    }
    \ \leq \
    \underbrace{
      \tilde O\Parens{
        \sqrt{
          \frac{\tmix \log(1/\delta)}{t}
        }
      }
    }_{\text{deviation bound}}
    \,.
  \]
  \FIREBRICK{Bound depends on $\tmix$, which may be unknown \emph{a
  priori}}.

  \onslide<2->
  \bigskip
  \textbf{Examples}:
  \begin{description}
    \item[Bayesian inference]
      \underline{Posterior means \& variances} via MCMC

    \item[Reinforcement learning]
      \underline{Mean action rewards} in an MDP

    \item[Supervised learning]
      \underline{Error rates of hypotheses} from non-iid data

  \end{description}

  \onslide<3->
  \begin{center}
    \BLUE{%
      Need \emph{observable} deviation bounds.
    }
  \end{center}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Observable deviation bounds from mixing time bounds?}

  Suppose an estimator $\tmixhat = \tmixhat(X_{1:t})$ of
  $\tmix$ satisfies:
  \[
    \P(\tmix \leq \tmixhat + \veps_t)
    \ \geq \
    1-\delta
    \,.
  \]
  \onslide<2->
  Then with probability at least $1-2\delta$,
  \[
    \Abs{
      \frac1t\sum_{i=1}^t f(X_i)
      -
      \E_\pi f
    }
    \ \leq \
    \tilde O\Parens{
      \sqrt{
        \frac{\parens{\tmixhat + \veps_t} \log(1/\delta)}{t}
      }
    }
    \,.
  \]

  \onslide<3->
  But $\tmixhat$ is computed from $X_{1:t}$, so
  \FIREBRICK{$\veps_t$ may also depend on $\tmix$}.

  \onslide<4->
  \begin{center}
    \FIREBRICK{%
      Deviation bounds for point estimators are insufficient.%
    }
    \\
    \BLUE{%
      Need \emph{(observable) confidence intervals} for $\tmix$.%
    }
  \end{center}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{What we do}

  \begin{enumerate}
    \item<2->
      Shift focus to \GREEN{relaxation time} $\trelax$ to enable
      spectral methods.

      \medskip

    \item<3->
      \textbf{Lower/upper bounds} on sample path length for point
      estimation of $\trelax$.

      \medskip

    \item<4->
      \textbf{New algorithm} for constructing confidence intervals for
      $\trelax$.

  \end{enumerate}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Relaxation time}

  \begin{itemize}
    \item
      Let $P$ be the \GREEN{transition operator} of the Markov chain,

      and let $\slem$ be its \GREEN{second-largest eigenvalue modulus}

      {\fns(i.e., largest eigenvalue modulus other than $1$)}.

      \medskip

    \item<2->
      \GREEN{Spectral gap}: $\gap := 1-\slem$.

      \GREEN{Relaxation time}: $\trelax := 1/\gap$.

      \[
        \Parens{ \trelax - 1 } \ln 2
        \ \leq \
        \tmix
        \ \leq \
        \trelax \ln \frac4{\pimin}
      \]
      for $\pimin := \min_{x \in \states} \pi(x)$.

      \onslide<3->
      \medskip
      Assumptions on $P$ ensure $\gap, \pimin \in (0,1)$.

  \end{itemize}

  \onslide<4->
  \begin{center}
    \BLUE{%
      \textbf{Spectral approach}: construct CI's for $\gap$ and $\pimin$.
    }
  \end{center}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Our results (point estimation)}

  We restrict to \GREEN{reversible Markov chains} on \GREEN{finite
  state spaces}.

  Let $d$ be the (known \emph{a priori}) cardinality of the state
  space $\states$.

  \begin{enumerate}
    \item<2->
      {\fns\textbf{Lower bound}:} \\
      To estimate $\gap$ within a constant multiplicative factor,
      \mbox{every algorithm} needs (w.p.~$1/4$) sample path of
      length
      \[
        \geq \
        \Omega\Parens{
          \frac{d \log d}{\gap} + \frac1{\pimin}
        }
        \,.
      \]

    \item<3->
      {\fns\textbf{Upper bound}:} \\
      Simple algorithm estimates $\gap$ and $\pimin$ within a constant
      multiplicative factor (w.h.p.) with sample path of length
      \begin{align*}
        \wt O\Parens{ \frac{\log d}{\pimin\gap^3} }
        & \quad
        \text{(for $\gap$)}
        \,,
        &
        \wt O\Parens{ \frac{\log d}{\pimin\gap} }
        & \quad
        \text{(for $\pimin$)}
        \,.
      \end{align*}

  \end{enumerate}

  \onslide<4->
  \begin{center}
    \FIREBRICK{%
      But point estimator $\not\Rightarrow$
      confidence interval.
    }
  \end{center}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Our results (confidence intervals)}

  \begin{enumerate}
    \item[3.]
      \textbf{New algorithm}:
      Given $\delta \in (0,1)$ and $X_{1:t}$ as input, constructs
      intervals $I_t^{\gap}$ and $I_t^{\pimin}$ such that
      \[
        \P\Parens{
          \gap \in I_t^{\gap}
        }
        \ \geq \ 1-\delta
        \quad
        \text{and}
        \quad
        \P\Parens{
          \pimin \in I_t^{\pimin}
        }
        \ \geq \ 1-\delta
        \,.
      \]

      Widths of intervals converge a.s.~to zero at
      $\sqrt{\frac{\log\log t}{t}}$ rate.

      \medskip

    \item[4.]<2->
      \textbf{Hybrid approach}:
      Use new algorithm to turn error bounds for point estimators into
      observable CI's.

      {\fns(This improves asymptotic rate for $\pimin$ interval.)}

  \end{enumerate}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Plug-in estimator}

  \begin{itemize}
    \item<2->
      Reversibility grants the symmetry of
      \[
        M
        \ := \
        \diag(\pi) P
        \ = \
        \Braces{ \P_{X_1 \sim \pi}(X_1=x,X_2=x') }_{x,x'\in\states}
      \]
      {\fns(doublet state probabilities in stationary chain)}.

      \smallskip

    \item<3->
      Moreover, eigenvalues of
      \[
        L
        \ := \
        \diag(\pi)^{-1/2} M \diag(\pi)^{-1/2}
      \]
      are real, and satisfy
      \begin{gather*}
        1
        \ = \ \lambda_1 \ > \ \lambda_2 \ \geq \ \dotsb \ \geq \
        \lambda_d \ > \ -1
        \,,
        \\
        \gap
        \ = \ 1 - \max\braces{ \lambda_2, \abs{\lambda_d} }
        \,.
      \end{gather*}

      \smallskip

    \item<4->
      \textbf{Plug-in estimator}:
      estimate $\pi$ and $M$ from $X_{1:t}$ (using empirical
      frequencies), then plug-in to formula for $\gap$.

  \end{itemize}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Chicken-and-egg problem}

  (Matrix) Chernoff bound {\fns(for Markov chains)} gives error bounds
  for estimates of $\pi$ and $M$ (and ultimately of $L$ and $\gap$):
  e.g., w.h.p.,
  \[
    \abs{\hatgap-\AZURE{\gap}}
    \ \leq \
    \norm{\wh L - L}
    \ \leq \
    O\Parens{
      \sqrt{\frac{\log(d)\log(t/\pimin)}{\AZURE{\gap}\pimin t}}
    }
    \,.
  \]
  \onslide<2->
  This has \FIREBRICK{\emph{inverse} dependence} on $\AZURE{\gap}$.

  \onslide<3->
  \begin{center}
    \FIREBRICK{%
      Can't ``solve the bound'' for $\AZURE{\gap}$
    }

    {\fns(unlike ``empirical Bernstein'' inequalities)}.

    \includegraphics[height=1.5in]{chicken-and-egg.jpg}
  \end{center}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Direct estimation of $P$}

  \textbf{Alternative}: directly estimate $P$ from $X_{1:t}$.

  \begin{itemize}
    \item<2->
      \textbf{Key advantage}: observable confidence intervals for $P$
      via ``empirical Bernstein'' inequality for martingales.

  \end{itemize}

  \onslide<3->
  \textbf{Two problems}:
  \begin{enumerate}
    \item<4->
      Without appealing to symmetry structure, can argue
      \[
        \norm{\wh{P} - P} \ \leq \ \veps
        \quad\Longrightarrow\quad
        \abs{\hatgap - \gap} \ \leq \ O(\veps^{1/(2d)})
        \,,
      \]
      but this implies \FIREBRICK{exponential slow-down in rate}.

      \medskip

    \item<5->
      Direct appeal to symmetry structure of
      \[
        L \ = \ \diag(\pi)^{1/2} P \diag(\pi)^{-1/2}
      \]
      gives bounds that depend on $\pi$, which is unknown.

  \end{enumerate}

  \onslide<6->
  \begin{center}
    \BLUE{%
      \textbf{Our approach}: \\
      Directly estimate $P$, and \emph{indirectly} estimate $\pi$ via
      $\wh P$.
    }
  \end{center}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Indirect estimation of $\pi$}

  \begin{enumerate}
    \item
      We ensure that $\wh P$ is transition operator for an ergodic chain
      {\fns(easy via Laplace smoothing)}.

      \medskip

    \item<2->
      \textbf{Key step}: estimate $\pi$ via $\wh P$ via \GREEN{group
      inverse} $\wh A^\#$ of $I - \wh P$.

      \medskip

      \begin{itemize}
        \item<3->
          $\wh A^\#$ contains ``virtually everything that one would want
          to know about the chain'' [with transition operator $\wh P$]
          {\fns(Meyer, 1975)}.

          \medskip

        \item<4->
          Reveals unique stationary distribution $\hat\pi$ w.r.t.~$\wh P$.

          \BLUE{This is our indirect estimate of $\pi$}.

          \medskip

        \item<5->
          Tells us how to bound $\norm{\hat\pi-\pi}_\infty$ in terms of
          $\norm{\wh P - P}$.

          \BLUE{Hence, from this, we construct a confidence interval
          for $\pi$.}

      \end{itemize}

%    \item
%      Use \GREEN{perturbation theory for eigenvalues of symmetric
%      operators} to get \BLUE{confidence interval for $\gap$ in terms
%      of confidence intervals for $P$ and $\pi$}.
%
  \end{enumerate}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Overall algorithm (outline)}

  \begin{enumerate}
    \item
      Form empirical estimate and confidence intervals for $P$

      {\fns(exploit Markov property \& ``empirical Bernstein''-type bounds)}.

      \medskip

    \item
      Form estimate and confidence intervals for $\pi$

      {\fns(via group inverse of $I - \wh P$)}.

      \medskip

    \item
      Form estimate and confidence interval for $\gap$

      {\fns(via confidence intervals for $\pi$ and $P$, \& eigenvalue perturbation theory)}.

  \end{enumerate}
\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Recap and future work}

  \begin{itemize}
    \item
      We resolve ``chicken-and-egg'' problem of observable
      confidence intervals for mixing time from a single sample path.

    \item<2->
      Strongly exploit Markov property and ergodicity in confidence
      intervals for $P$ and $\pi$.

    \item<3->
      \textbf{Problem \#1}:
      \textcolor{britishracinggreen}{%
        close gap between lower and upper bounds on sample path length
        (for point estimation).%
      }

    \item<4->
      \textbf{Problem \#2}:
      \textcolor{britishracinggreen}{%
        overcome computational bottlenecks from matrix operations.%
      }

    \item<5->
      \textbf{Problem \#3}:
      \textcolor{boldgreen}{%
        handle large/continuous state spaces under suitable
        assumptions.%
      }

  \end{itemize}

\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
  \begin{center}
    \Huge
    Thanks!
  \end{center}
\end{frame}

%-------------------------------------------------------------------------------

\end{document}


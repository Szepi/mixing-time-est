We thank the referees for the helpful comments. We'll incorporate the suggestions in a revision of the paper, especially where more detailed explanations are needed.

The following two concerns were common to multiple reviews.

Tightness of bounds: We don't claim tightness of the bounds -- they are indeed rather loose -- but our results give the first non-trivial (upper and lower) bounds in this challenging setting. Our work is motivated by practical applications where fully data-dependent intervals were sought but not actually achieved. Our results are a first step towards a practical solution: we demonstrate feasibility and also prove limitations on what is possible.

Relaxation vs mixing time: In the revision, we will be more explicit about estimating the relaxation time rather than mixing time proper (throughout the abstract and paper). We fully agree that this is a very important distinction that should be emphasized, and we'll amend the overstated claims throughout.

Additional responses to individual reviews follow.

Reviewer_1

2. We'll make this edit in the revision.

3. Qualitatively, Theorems 1 and 2 show that if n is less than some function of (d,pi,gamma), then for any estimator based on a sample path of length n, there is some Markov chain such that the error is large. We'll add additional clarifying interpretations in the revision.

4. If n > max{16C^2,4C}/(pi_* gamma_*), then the RHS in (4) (without the logarithmic factor) is less than pi_*/2.

If n > max{16C^2,4C}/(pi_* gamma_*^3), then the RHS in (5) (without the logarithmic factor) is less than gamma_*/2.

We will add these details in the revision.

5. Combining Theorems 1 and 2 with a standard argument yields the lower bound in the introduction. We will include the details in the revision.

6. Regarding Theorem 1: Note that "c" is an absolute constant in the theorem, not something that can be chosen arbitrarily. In particular, one value that works in the theorem is c=1/4.

There were some minor numerical calculation errors in the proof of Theorem 1, which we can clarify here. We need to change the "1/6" in the theorem statement and proof to "1/8". Furthermore, in the proof, \bar\pi should be taken in (0,1/4), and we should set \varepsilon := \bar\pi. Observe that \pi^{(2)} = (1/(1+2\varepsilon),2\varepsilon/(1+2\varepsilon)). Thus, \pi_\star \geq \bar\pi in both P^{(1)} and P^{(2)}. To guarantee |\hat\gamma_\star - \gamma_\star| < 1/8, it must be possible to distinguish the two Markov chains. The rest of the proof is correct as is.

7. To conclude Theorem 2, it follows the second-moment method (e.g., Paley-Zygmund) and the moment bounds that, with probability > 1/4, the earliest time in which a sample path visits all d states is at least c d log(d) / \bar\gamma.  This implies that an estimator using a shorter sample path cannot distinguish P^0 from P^i for some i (with probability > 1/4), and hence must have large estimation error. We'll include these details in the revision.

Reviewer_2

Theorem 5: We get an improvement when the min in the bound for |\hat{V}| is achieved by the first term. A sheepish caveat: this is not always the case for all sufficiently large n, due to the sqrt{log(n)} term in the numerator, but will be true for a very large range of n. There is more discussion in Section 5.

\tilde{O}: We'll adopt this more standard use in the revision.

kappa: It is defined just above Theorem 4.

Theorem 2 (line 188): Indeed, including an extra constant factor in the claim resolves this issue.

Theorem 2 (line 958): An estimator cannot rule out P^i without having visited state i. We'll clarify this point in the revision.

Reviewer_7

Perfect sampling: Indeed, this line of work is related, although these methods generally do not apply in our setting of estimation from a single sample path. We'll discuss this in the revision.
